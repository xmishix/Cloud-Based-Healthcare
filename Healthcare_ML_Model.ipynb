{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xd6EOe6BNby",
        "outputId": "661af63f-7246-4331-e5de-06a5e69fa54f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORT LIBRARIES"
      ],
      "metadata": {
        "id": "-lUezlYFFKUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "from sklearn.metrics import (roc_auc_score, average_precision_score, accuracy_score,\n",
        "                             precision_score, recall_score, f1_score, confusion_matrix,\n",
        "                             classification_report, roc_curve, precision_recall_curve)\n",
        "import joblib\n",
        "import math\n",
        "import time\n",
        "import json\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "WuEQ2T2-Eu1e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except Exception:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except Exception:\n",
        "    SHAP_AVAILABLE = False\n"
      ],
      "metadata": {
        "id": "6s06RdVyE7xY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOAD DATA"
      ],
      "metadata": {
        "id": "yd0y0SMJFUX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = \"/content/drive/MyDrive/final dataset.csv\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/readmission_output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.2\n",
        "CV_FOLDS = 3\n",
        "N_JOBS = -1\n",
        "SCORING_METRIC = \"roc_auc\"\n",
        "RISK_THRESHOLD = 0.5"
      ],
      "metadata": {
        "id": "zzuGQuaZFXX1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_target_col(df):\n",
        "    \"\"\"Find a readmission-like target column name in dataframe.\"\"\"\n",
        "    candidates = [c for c in df.columns if any(k in c.lower() for k in ['readmit','readmission','readmitted','readmit_30','readmission_30'])]\n",
        "    return candidates[0] if candidates else None\n",
        "\n",
        "def remove_existing_risk_cols(df):\n",
        "    risk_cols = [c for c in df.columns if 'risk' in c.lower()]\n",
        "    if risk_cols:\n",
        "        print(\"Removing existing risk columns:\", risk_cols)\n",
        "        df = df.drop(columns=risk_cols, errors='ignore')\n",
        "    return df\n",
        "\n",
        "def safe_to_numeric(s):\n",
        "    try:\n",
        "        return pd.to_numeric(s, errors='coerce')\n",
        "    except Exception:\n",
        "        return s"
      ],
      "metadata": {
        "id": "INwAvVYuFzje"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOAD & CLEAN"
      ],
      "metadata": {
        "id": "OD4xVIYoF6J8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading data from:\", DATA_PATH)\n",
        "df_raw = pd.read_csv(DATA_PATH, dtype=object)\n",
        "print(\"Initial shape:\", df_raw.shape)\n",
        "df_raw = remove_existing_risk_cols(df_raw)\n",
        "\n",
        "# Detect target\n",
        "target_col = find_target_col(df_raw)\n",
        "if target_col is None:\n",
        "    raise RuntimeError(\"No readmission target column found. Please include a column name containing 'readmit' or 'readmission'.\")\n",
        "\n",
        "print(\"Detected target column:\", target_col)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwQleiSdF8fv",
        "outputId": "3c59d3e0-8719-48b1-96da-093c65e04206"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from: /content/drive/MyDrive/final dataset.csv\n",
            "Initial shape: (5000, 37)\n",
            "Detected target column: Readmission\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJW-yCuHyJ7u",
        "outputId": "77c55b0a-fdd1-4e49-cb13-36de1311107f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Patient Name', 'Admission ID', 'Age', 'Sex', 'Weight',\n",
              "       'Admission Date', 'Admission Time', 'Consultant Doctor Name',\n",
              "       'Doctor Name', 'Doctor ID', 'Problem Type', 'Discharge Date',\n",
              "       'Discharge Time', 'Readmission', 'Blood Pressure', 'Insulin',\n",
              "       'Blood Group', 'Cholesterol', 'Platelets', 'Diabetics',\n",
              "       'Problem Description', 'Nurse Name', 'Patient Phone Number',\n",
              "       'Patient Mail ID', 'weather', 'air_quality_index', 'social_event_count',\n",
              "       'Hemoglobin (g/dL)', 'WBC Count (10^9/L)', 'Platelet Count (10^9/L)',\n",
              "       'Urine Protein (mg/dL)', 'Urine Glucose (mg/dL)', 'ECG Result',\n",
              "       'Pulse Rate (bpm)', 'State', 'City', 'Location'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NORMALIZE THE VALUES"
      ],
      "metadata": {
        "id": "x4sn3EUGGDPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_raw.copy()\n",
        "df[target_col] = df[target_col].astype(str).str.strip().str.lower().map({\n",
        "    'yes':'1','y':'1','true':'1','1':'1','no':'0','n':'0','false':'0','0':'0'\n",
        "})\n",
        "df[target_col] = pd.to_numeric(df[target_col], errors='coerce')\n",
        "print(\"Target value counts (including NaN):\")\n",
        "print(df[target_col].value_counts(dropna=False).to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0717dLroF-RX",
        "outputId": "4d85a245-be6f-4786-c670-37d651c7cb73"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target value counts (including NaN):\n",
            "Readmission\n",
            "0    2500\n",
            "1    2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DROP ROWS WITH MISSING VALUES"
      ],
      "metadata": {
        "id": "ODUhsSF6GOde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "before = len(df)\n",
        "df = df[df[target_col].notna()].copy()\n",
        "print(f\"Dropped {before - len(df)} rows with missing target. Remaining: {len(df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKNz5H1lGL2_",
        "outputId": "d0e06f6a-1c5f-40c9-b761-a124cbc51b89"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropped 0 rows with missing target. Remaining: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPLORATORY DATA ANALYTICS"
      ],
      "metadata": {
        "id": "dUXkkedTGZ7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Basic EDA ---\")\n",
        "print(\"Columns:\", list(df.columns))\n",
        "print(\"\\nNumeric sample summary (attempt coercion):\")\n",
        "# attempt to coerce commonly numeric columns for summary\n",
        "sample_numeric_cols = []\n",
        "for c in df.columns:\n",
        "    coerced = pd.to_numeric(df[c], errors='coerce')\n",
        "    if coerced.notna().sum() > max(10, 0.01 * len(df)):  # if column has many numeric-like values\n",
        "        sample_numeric_cols.append(c)\n",
        "print(\"Numeric-candidate columns:\", sample_numeric_cols[:20])\n",
        "if 'age' in [c.lower() for c in df.columns]:\n",
        "    ac = [c for c in df.columns if c.lower()=='age'][0]\n",
        "    print(df[ac].astype(str).describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJLQ0s-cGYWH",
        "outputId": "5c9dd93b-1a6c-41ef-cffa-aa20704559c3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Basic EDA ---\n",
            "Columns: ['Patient Name', 'Admission ID', 'Age', 'Sex', 'Weight', 'Admission Date', 'Admission Time', 'Consultant Doctor Name', 'Doctor Name', 'Doctor ID', 'Problem Type', 'Discharge Date', 'Discharge Time', 'Readmission', 'Blood Pressure', 'Insulin', 'Blood Group', 'Cholesterol', 'Platelets', 'Diabetics', 'Problem Description', 'Nurse Name', 'Patient Phone Number', 'Patient Mail ID', 'weather', 'air_quality_index', 'social_event_count', 'Hemoglobin (g/dL)', 'WBC Count (10^9/L)', 'Platelet Count (10^9/L)', 'Urine Protein (mg/dL)', 'Urine Glucose (mg/dL)', 'ECG Result', 'Pulse Rate (bpm)', 'State', 'City', 'Location']\n",
            "\n",
            "Numeric sample summary (attempt coercion):\n",
            "Numeric-candidate columns: ['Age', 'Weight', 'Readmission', 'Cholesterol', 'Platelets', 'Patient Phone Number', 'air_quality_index', 'social_event_count', 'Hemoglobin (g/dL)', 'WBC Count (10^9/L)', 'Platelet Count (10^9/L)', 'Urine Protein (mg/dL)', 'Urine Glucose (mg/dL)', 'Pulse Rate (bpm)']\n",
            "count     5000\n",
            "unique      66\n",
            "top         56\n",
            "freq        96\n",
            "Name: Age, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def quick_plots(df_local, target):\n",
        "    try:\n",
        "        sns.set()\n",
        "        plt.figure(figsize=(5,3))\n",
        "        sns.countplot(x=target, data=df_local)\n",
        "        plt.title(\"Target distribution\")\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(\"Skipping quick_plots due to:\", e)"
      ],
      "metadata": {
        "id": "kptGwdaeGkvT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Feature engineering ---\")\n",
        "df_fe = df.copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfdGfgRhGoPy",
        "outputId": "214e656f-bd7a-4bbb-d707-773f61630148"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Feature engineering ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cols_lower = {c.lower(): c for c in df_fe.columns}"
      ],
      "metadata": {
        "id": "7AS2lQUEGoKA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_col = None\n",
        "for candidate in ['age','patient_age','age_years']:\n",
        "    if candidate in cols_lower:\n",
        "        age_col = cols_lower[candidate]\n",
        "        break\n",
        "\n",
        "if age_col:\n",
        "    df_fe[age_col] = pd.to_numeric(df_fe[age_col], errors='coerce')\n",
        "    df_fe['age_bucket'] = pd.cut(df_fe[age_col], bins=[0,30,50,65,80,200], labels=['<=30','31-50','51-65','66-80','80+'])\n",
        "    print(\"Created age_bucket from\", age_col)\n",
        "else:\n",
        "    print(\"No age column found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaMnykbsGvu0",
        "outputId": "a97020dd-47fd-4d16-b9e8-cfc70cfc29cb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created age_bucket from Age\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "admit_col = None\n",
        "discharge_col = None\n",
        "for c in df_fe.columns:\n",
        "    low = c.lower()\n",
        "    if 'admit' in low and admit_col is None:\n",
        "        admit_col = c\n",
        "    if 'discharg' in low and discharge_col is None:\n",
        "        discharge_col = c\n",
        "\n",
        "if admit_col:\n",
        "    df_fe[admit_col] = pd.to_datetime(df_fe[admit_col], errors='coerce')\n",
        "if discharge_col:\n",
        "    df_fe[discharge_col] = pd.to_datetime(df_fe[discharge_col], errors='coerce')\n",
        "\n",
        "if admit_col and discharge_col:\n",
        "    df_fe['los_days'] = (df_fe[discharge_col] - df_fe[admit_col]).dt.days\n",
        "    df_fe.loc[(df_fe['los_days'] < 0) | (df_fe['los_days'] > 3650), 'los_days'] = np.nan\n",
        "    print(\"Computed los_days from\", admit_col, \"and\", discharge_col)\n",
        "else:\n",
        "    print(\"Admission/discharge dates not both found -> skipping LOS feature.\")\n",
        "\n",
        "# Comorbidity columns detection and count\n",
        "comorbidity_keywords = ['diabetes','hypertension','hyperten','cancer','copd','asthma','heart','renal','kidney','stroke']\n",
        "comorb_cols = [c for c in df_fe.columns if any(k in c.lower() for k in comorbidity_keywords)]\n",
        "print(\"Detected comorbidity-like columns:\", comorb_cols[:20])\n",
        "for c in comorb_cols:\n",
        "    df_fe[c] = pd.to_numeric(df_fe[c], errors='coerce').fillna(df_fe[c].astype(str).str.lower().map({'yes':1,'y':1,'true':1,'1':1,'no':0,'n':0,'false':0}))\n",
        "    df_fe[c] = pd.to_numeric(df_fe[c], errors='coerce').fillna(0).astype(int)\n",
        "if comorb_cols:\n",
        "    df_fe['comorbidity_count'] = df_fe[comorb_cols].sum(axis=1)\n",
        "else:\n",
        "    df_fe['comorbidity_count'] = 0\n",
        "\n",
        "# Date-based features (admit weekday/month)\n",
        "if admit_col:\n",
        "    df_fe['admit_weekday'] = df_fe[admit_col].dt.weekday\n",
        "    df_fe['admit_month'] = df_fe[admit_col].dt.month\n",
        "\n",
        "# Coerce some lab-like columns to numeric if present\n",
        "lab_keywords = ['hemoglobin','hb','wbc','platelet','creatinine','cholesterol','glucose','pulse']\n",
        "for c in df_fe.columns:\n",
        "    if any(k in c.lower() for k in lab_keywords):\n",
        "        df_fe[c] = pd.to_numeric(df_fe[c], errors='coerce')\n",
        "\n",
        "# Drop personal-identifying or long-text columns\n",
        "drop_candidates = [c for c in df_fe.columns if any(k in c.lower() for k in ['name','address','phone','mail','email','notes','description','image','photo','url'])]\n",
        "if drop_candidates:\n",
        "    print(\"Dropping candidate PII / long-text columns (count={}): {}\".format(len(drop_candidates), drop_candidates[:10]))\n",
        "    df_fe = df_fe.drop(columns=drop_candidates, errors='ignore')\n",
        "\n",
        "print(\"Feature engineering completed. Shape:\", df_fe.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRfCiO7_G5Mx",
        "outputId": "8a92057e-cf5b-4f6a-b0d9-9b5cb5eb6b0f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Admission/discharge dates not both found -> skipping LOS feature.\n",
            "Detected comorbidity-like columns: []\n",
            "Dropping candidate PII / long-text columns (count=7): ['Patient Name', 'Consultant Doctor Name', 'Doctor Name', 'Problem Description', 'Nurse Name', 'Patient Phone Number', 'Patient Mail ID']\n",
            "Feature engineering completed. Shape: (5000, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PREPARE FEATURE MATRIX"
      ],
      "metadata": {
        "id": "3QaPCyLEHInJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPreparing feature matrix X and target y...\")\n",
        "# Ensure target is int\n",
        "df_fe[target_col] = df_fe[target_col].astype(int)\n",
        "X = df_fe.drop(columns=[target_col], errors='ignore')\n",
        "y = df_fe[target_col]\n",
        "\n",
        "# Identify numeric and categorical features\n",
        "numeric_cols = X.select_dtypes(include=['int64','float64']).columns.tolist()\n",
        "# Treat small-cardinality object columns as categorical\n",
        "cat_cols = [c for c in X.columns if (X[c].dtype == object or X[c].nunique() < 50) and c not in numeric_cols]\n",
        "\n",
        "# Remove columns that are obviously datetime objects from feature lists\n",
        "numeric_cols = [c for c in numeric_cols if not np.issubdtype(type(X[c].dtype), np.datetime64)]\n",
        "# Make sure we don't include admit/discharge date columns directly\n",
        "for dtcol in [admit_col, discharge_col]:\n",
        "    if dtcol in cat_cols: cat_cols.remove(dtcol)\n",
        "    if dtcol in numeric_cols: numeric_cols.remove(dtcol)\n",
        "\n",
        "print(\"Numeric cols count:\", len(numeric_cols))\n",
        "print(\"Categorical cols count:\", len(cat_cols))\n",
        "\n",
        "# Preprocessor\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, numeric_cols),\n",
        "    ('cat', categorical_transformer, cat_cols)\n",
        "], remainder='drop', sparse_threshold=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eORIP4-VHQcG",
        "outputId": "5efa39aa-c9d7-453a-abb4-b36567e5d189"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preparing feature matrix X and target y...\n",
            "Numeric cols count: 9\n",
            "Categorical cols count: 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SPLIT"
      ],
      "metadata": {
        "id": "561AV55qHZad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTrain/test split (test_size={})\".format(TEST_SIZE))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE,\n",
        "                                                    random_state=RANDOM_STATE, stratify=y)\n",
        "print(\"Train size:\", X_train.shape, \"Test size:\", X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlWDqD5PHbbc",
        "outputId": "83e39f1b-61cb-4e9e-dec9-47a641eca014"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train/test split (test_size=0.2)\n",
            "Train size: (4000, 31) Test size: (1000, 31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL TRAINING & HYPER TUNING"
      ],
      "metadata": {
        "id": "7NSPIy7lHgmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "\n",
        "def evaluate_pipeline(name, pipeline, X_test, y_test):\n",
        "    y_prob = pipeline.predict_proba(X_test)[:,1]\n",
        "    y_pred = (y_prob >= RISK_THRESHOLD).astype(int)\n",
        "    metrics = {}\n",
        "    metrics['roc_auc'] = roc_auc_score(y_test, y_prob)\n",
        "    metrics['pr_auc']  = average_precision_score(y_test, y_prob)\n",
        "    metrics['accuracy'] = accuracy_score(y_test, y_pred)\n",
        "    metrics['precision'] = precision_score(y_test, y_pred, zero_division=0)\n",
        "    metrics['recall'] = recall_score(y_test, y_pred, zero_division=0)\n",
        "    metrics['f1'] = f1_score(y_test, y_pred, zero_division=0)\n",
        "    metrics['confusion_matrix'] = confusion_matrix(y_test, y_pred)\n",
        "    print(f\"\\n{name} Evaluation:\")\n",
        "    print(f\"ROC-AUC: {metrics['roc_auc']:.4f} | PR-AUC: {metrics['pr_auc']:.4f} | Acc: {metrics['accuracy']:.4f} | F1: {metrics['f1']:.4f}\")\n",
        "    print(\"Confusion matrix:\\n\", metrics['confusion_matrix'])\n",
        "    print(\"Classification report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "MPns63jcHkQt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOGIESTIC REGRESSION"
      ],
      "metadata": {
        "id": "1S7XAKorHo8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTraining Logistic Regression with GridSearchCV...\")\n",
        "lr_pipe = Pipeline(steps=[('pre', preprocessor),\n",
        "                          ('clf', LogisticRegression(solver='saga', max_iter=3000, class_weight='balanced', random_state=RANDOM_STATE))])\n",
        "lr_param_grid = {\n",
        "    'clf__C': [0.01, 0.1, 1.0, 5.0],\n",
        "    'clf__penalty': ['l2']  # keep L2 for stability; L1 is also possible with saga\n",
        "}\n",
        "lr_search = GridSearchCV(lr_pipe, lr_param_grid, scoring=SCORING_METRIC, cv=StratifiedKFold(n_splits=CV_FOLDS), n_jobs=N_JOBS, verbose=1)\n",
        "lr_search.fit(X_train, y_train)\n",
        "print(\"LR best params:\", lr_search.best_params_)\n",
        "lr_best = lr_search.best_estimator_\n",
        "results['LogisticRegression'] = evaluate_pipeline(\"Logistic Regression\", lr_best, X_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbcSvKt4Hp7M",
        "outputId": "ccc551ba-d4b7-4955-b896-f21bbd070cd7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Logistic Regression with GridSearchCV...\n",
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
            "LR best params: {'clf__C': 0.1, 'clf__penalty': 'l2'}\n",
            "\n",
            "Logistic Regression Evaluation:\n",
            "ROC-AUC: 0.4805 | PR-AUC: 0.4975 | Acc: 0.4860 | F1: 0.4787\n",
            "Confusion matrix:\n",
            " [[250 250]\n",
            " [264 236]]\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4864    0.5000    0.4931       500\n",
            "           1     0.4856    0.4720    0.4787       500\n",
            "\n",
            "    accuracy                         0.4860      1000\n",
            "   macro avg     0.4860    0.4860    0.4859      1000\n",
            "weighted avg     0.4860    0.4860    0.4859      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RANDOM FOREST"
      ],
      "metadata": {
        "id": "cHglUZIUJyR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTraining Random Forest with RandomizedSearchCV...\")\n",
        "rf_pipe = Pipeline(steps=[('pre', preprocessor),\n",
        "                          ('clf', RandomForestClassifier(class_weight='balanced', random_state=RANDOM_STATE, n_jobs=1))])\n",
        "rf_param_dist = {\n",
        "    'clf__n_estimators': [200, 400, 600],\n",
        "    'clf__max_depth': [6, 10, 15, None],\n",
        "    'clf__min_samples_split': [2, 5, 10],\n",
        "    'clf__min_samples_leaf': [1, 2, 4],\n",
        "    'clf__max_features': ['sqrt', 0.3, 0.6]\n",
        "}\n",
        "rf_search = RandomizedSearchCV(rf_pipe, rf_param_dist, n_iter=20, scoring=SCORING_METRIC,\n",
        "                               cv=StratifiedKFold(n_splits=CV_FOLDS), random_state=RANDOM_STATE, n_jobs=N_JOBS, verbose=1)\n",
        "rf_search.fit(X_train, y_train)\n",
        "print(\"RF best params:\", rf_search.best_params_)\n",
        "rf_best = rf_search.best_estimator_\n",
        "results['RandomForest'] = evaluate_pipeline(\"Random Forest\", rf_best, X_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JARwtEo6J0om",
        "outputId": "b553ab1a-cb0d-4e41-900c-8482c108e343"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Random Forest with RandomizedSearchCV...\n",
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
            "RF best params: {'clf__n_estimators': 600, 'clf__min_samples_split': 5, 'clf__min_samples_leaf': 4, 'clf__max_features': 'sqrt', 'clf__max_depth': None}\n",
            "\n",
            "Random Forest Evaluation:\n",
            "ROC-AUC: 0.4834 | PR-AUC: 0.5005 | Acc: 0.4970 | F1: 0.4830\n",
            "Confusion matrix:\n",
            " [[262 238]\n",
            " [265 235]]\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4972    0.5240    0.5102       500\n",
            "           1     0.4968    0.4700    0.4830       500\n",
            "\n",
            "    accuracy                         0.4970      1000\n",
            "   macro avg     0.4970    0.4970    0.4966      1000\n",
            "weighted avg     0.4970    0.4970    0.4966      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XG BOOST"
      ],
      "metadata": {
        "id": "75a1bM6Gbig9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if XGBOOST_AVAILABLE:\n",
        "    print(\"\\nTraining XGBoost with RandomizedSearchCV...\")\n",
        "    xgb_pipe = Pipeline(steps=[('pre', preprocessor),\n",
        "                               ('clf', xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=RANDOM_STATE))])\n",
        "    # compute scale_pos_weight for imbalance\n",
        "    pos = int(y_train.sum())\n",
        "    neg = len(y_train) - pos\n",
        "    scale_pos_weight = (neg / pos) if pos>0 else 1.0\n",
        "    xgb_param_dist = {\n",
        "        'clf__n_estimators': [100, 200, 400],\n",
        "        'clf__max_depth': [3, 5, 7],\n",
        "        'clf__learning_rate': [0.01, 0.05, 0.1],\n",
        "        'clf__subsample': [0.6, 0.8, 1.0],\n",
        "        'clf__colsample_bytree': [0.6, 0.8, 1.0],\n",
        "        'clf__scale_pos_weight': [scale_pos_weight]\n",
        "    }\n",
        "    xgb_search = RandomizedSearchCV(xgb_pipe, xgb_param_dist, n_iter=20, scoring=SCORING_METRIC,\n",
        "                                    cv=StratifiedKFold(n_splits=CV_FOLDS), random_state=RANDOM_STATE, n_jobs=N_JOBS, verbose=1)\n",
        "    xgb_search.fit(X_train, y_train)\n",
        "    print(\"XGB best params:\", xgb_search.best_params_)\n",
        "    xgb_best = xgb_search.best_estimator_\n",
        "    results['XGBoost'] = evaluate_pipeline(\"XGBoost\", xgb_best, X_test, y_test)\n",
        "else:\n",
        "    print(\"\\nXGBoost not available — training HistGradientBoosting as a strong fallback.\")\n",
        "    hgb_pipe = Pipeline(steps=[('pre', preprocessor),\n",
        "                               ('clf', HistGradientBoostingClassifier(random_state=RANDOM_STATE))])\n",
        "    hgb_param_dist = {\n",
        "        'clf__max_iter': [100, 200],\n",
        "        'clf__max_leaf_nodes': [15, 31, 63]\n",
        "    }\n",
        "    hgb_search = RandomizedSearchCV(hgb_pipe, hgb_param_dist, n_iter=6, scoring=SCORING_METRIC,\n",
        "                                    cv=StratifiedKFold(n_splits=CV_FOLDS), random_state=RANDOM_STATE, n_jobs=N_JOBS, verbose=1)\n",
        "    hgb_search.fit(X_train, y_train)\n",
        "    hgb_best = hgb_search.best_estimator_\n",
        "    results['HistGB'] = evaluate_pipeline(\"HistGradientBoosting\", hgb_best, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXqwDhpZbLfR",
        "outputId": "ce0d7140-6ff8-46b7-c05e-fb58da4fa9d7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "XGBoost not available — training HistGradientBoosting as a strong fallback.\n",
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
            "\n",
            "HistGradientBoosting Evaluation:\n",
            "ROC-AUC: 0.4920 | PR-AUC: 0.5042 | Acc: 0.5020 | F1: 0.4866\n",
            "Confusion matrix:\n",
            " [[266 234]\n",
            " [264 236]]\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5019    0.5320    0.5165       500\n",
            "           1     0.5021    0.4720    0.4866       500\n",
            "\n",
            "    accuracy                         0.5020      1000\n",
            "   macro avg     0.5020    0.5020    0.5016      1000\n",
            "weighted avg     0.5020    0.5020    0.5016      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL SELECTION"
      ],
      "metadata": {
        "id": "9NJmEUDJfsXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Model selection by ROC-AUC ---\")\n",
        "best_name = max(results.items(), key=lambda kv: kv[1]['roc_auc'])[0]\n",
        "print(\"Model scores (ROC-AUC):\")\n",
        "for k,v in results.items():\n",
        "    print(f\" - {k}: {v['roc_auc']:.4f} (Acc={v['accuracy']:.4f}, F1={v['f1']:.4f})\")\n",
        "print(\"Selected best model:\", best_name)\n",
        "\n",
        "if best_name == 'LogisticRegression':\n",
        "    best_pipeline = lr_best\n",
        "elif best_name == 'RandomForest':\n",
        "    best_pipeline = rf_best\n",
        "elif best_name == 'XGBoost' and XGBOOST_AVAILABLE:\n",
        "    best_pipeline = xgb_best\n",
        "elif 'XGBoost' not in results and 'HistGB' in results:\n",
        "    best_pipeline = hgb_best\n",
        "else:\n",
        "    # fallback\n",
        "    best_pipeline = rf_best if 'rf_best' in locals() else lr_best\n",
        "\n",
        "# Save best pipeline\n",
        "best_model_path = os.path.join(OUTPUT_DIR, \"best_readmission_pipeline.joblib\")\n",
        "joblib.dump(best_pipeline, best_model_path)\n",
        "print(\"Saved best pipeline to:\", best_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDTFzsIAfrP9",
        "outputId": "6dfd9857-f503-45d3-c6a5-b334c255300e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model selection by ROC-AUC ---\n",
            "Model scores (ROC-AUC):\n",
            " - LogisticRegression: 0.4805 (Acc=0.4860, F1=0.4787)\n",
            " - RandomForest: 0.4834 (Acc=0.4970, F1=0.4830)\n",
            " - HistGB: 0.4920 (Acc=0.5020, F1=0.4866)\n",
            "Selected best model: HistGB\n",
            "Saved best pipeline to: /content/drive/MyDrive/readmission_output/best_readmission_pipeline.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPLANABILITY"
      ],
      "metadata": {
        "id": "6siaEar3gDGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nComputing feature names (post-preprocessing)...\")\n",
        "# try to reconstruct feature names\n",
        "feature_names = []\n",
        "try:\n",
        "    pre = best_pipeline.named_steps['pre']\n",
        "    # ColumnTransformer.get_feature_names_out is available on modern sklearn\n",
        "    try:\n",
        "        feature_names = pre.get_feature_names_out()\n",
        "    except Exception:\n",
        "        # attempt manual concatenation\n",
        "        num_names = numeric_cols\n",
        "        cat_encoder = pre.named_transformers_['cat'].named_steps['onehot'] if 'cat' in pre.named_transformers_ else None\n",
        "        if cat_encoder is not None:\n",
        "            cat_names = cat_encoder.get_feature_names_out(cat_cols)\n",
        "            feature_names = list(num_names) + list(cat_names)\n",
        "        else:\n",
        "            feature_names = list(num_names) + list(cat_cols)\n",
        "except Exception as e:\n",
        "    print(\"Could not extract feature names automatically:\", e)\n",
        "    feature_names = list(X.columns)\n",
        "\n",
        "# Feature importances for tree models\n",
        "clf = best_pipeline.named_steps.get('clf', None)\n",
        "if hasattr(clf, \"feature_importances_\"):\n",
        "    try:\n",
        "        importances = clf.feature_importances_\n",
        "        # align length\n",
        "        fn = feature_names if len(feature_names)==len(importances) else [f\"f{i}\" for i in range(len(importances))]\n",
        "        imp_df = pd.DataFrame({'feature': fn, 'importance': importances}).sort_values('importance', ascending=False)\n",
        "        imp_df.to_csv(os.path.join(OUTPUT_DIR, \"feature_importances.csv\"), index=False)\n",
        "        print(\"Saved feature importances to output directory.\")\n",
        "    except Exception as e:\n",
        "        print(\"Could not produce feature importances:\", e)\n",
        "else:\n",
        "    print(\"Best model does not expose feature_importances_ (likely LR).\")\n",
        "\n",
        "# SHAP explanations (if available)\n",
        "if SHAP_AVAILABLE:\n",
        "    try:\n",
        "        print(\"Computing SHAP values (sample) — this can take time...\")\n",
        "        # generate background by sampling training set after preprocessing\n",
        "        # Use best_pipeline to transform\n",
        "        preproc = best_pipeline.named_steps['pre']\n",
        "        # take a small background\n",
        "        bg = X_train.sample(min(200, len(X_train)), random_state=RANDOM_STATE)\n",
        "        X_bg = preproc.transform(bg)\n",
        "        # create shap explainer for estimator\n",
        "        explainer = shap.Explainer(best_pipeline.named_steps['clf'], X_bg, feature_names=feature_names)\n",
        "        sample_for_shap = preproc.transform(X_test.sample(min(100, len(X_test)), random_state=RANDOM_STATE))\n",
        "        shap_vals = explainer(sample_for_shap)\n",
        "        # save summary plot\n",
        "        shap.summary_plot(shap_vals, feature_names=feature_names, show=False)\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, \"shap_summary.png\"), bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(\"Saved SHAP summary plot.\")\n",
        "    except Exception as e:\n",
        "        print(\"SHAP failed:\", e)\n",
        "else:\n",
        "    print(\"SHAP not installed; skip SHAP analysis.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-mG2H9RfPCm",
        "outputId": "a2c63e31-ac08-4abe-a797-54331074c8b4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Computing feature names (post-preprocessing)...\n",
            "Best model does not expose feature_importances_ (likely LR).\n",
            "SHAP not installed; skip SHAP analysis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SCORE FULL DATA"
      ],
      "metadata": {
        "id": "9rf7KSlVgOby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nScoring full dataset and saving predictions...\")\n",
        "X_full = X.copy()\n",
        "pred_probs = best_pipeline.predict_proba(X_full)[:,1]\n",
        "df_out = df_fe.copy()\n",
        "df_out['Predicted_Risk_Score'] = pred_probs\n",
        "df_out['Predicted_Readmission'] = np.where(df_out['Predicted_Risk_Score'] >= RISK_THRESHOLD, 'Yes', 'No')\n",
        "\n",
        "out_csv = os.path.join(OUTPUT_DIR, \"readmission_predictions_with_features.csv\")\n",
        "df_out.to_csv(out_csv, index=False)\n",
        "print(\"Saved predictions CSV to:\", out_csv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UB2qbX7ngQnc",
        "outputId": "5441b3c9-c5c1-449f-baea-aea197412c4f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scoring full dataset and saving predictions...\n",
            "Saved predictions CSV to: /content/drive/MyDrive/readmission_output/readmission_predictions_with_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PLOT ROC AND PR CURVES"
      ],
      "metadata": {
        "id": "9LuIx-2egeWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Plotting ROC and PR curves for best model on test set...\")\n",
        "y_prob_test = best_pipeline.predict_proba(X_test)[:,1]\n",
        "fpr, tpr, _ = roc_curve(y_test, y_prob_test)\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_prob_test)\n",
        "roc_auc_val = roc_auc_score(y_test, y_prob_test)\n",
        "pr_auc_val = average_precision_score(y_test, y_prob_test)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(fpr, tpr, label=f'ROC (AUC={roc_auc_val:.3f})')\n",
        "plt.plot([0,1],[0,1],'--', color='gray')\n",
        "plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC Curve'); plt.legend(); plt.grid(True)\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"roc_curve.png\"))\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(recall, precision, label=f'PR (AUC={pr_auc_val:.3f})')\n",
        "plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('Precision-Recall Curve'); plt.legend(); plt.grid(True)\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"pr_curve.png\"))\n",
        "plt.close()\n",
        "\n",
        "print(\"Saved ROC and PR plots to output directory.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5onZPExzgdqN",
        "outputId": "5b60b1b7-3641-44f9-a0e4-adb8d23bd0a7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting ROC and PR curves for best model on test set...\n",
            "Saved ROC and PR plots to output directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATE INTERPRETABLE RISK SCORING SYSTEM MAPPING"
      ],
      "metadata": {
        "id": "OmyH_QzZgzup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def risk_group(score):\n",
        "    if score > 0.7:\n",
        "        return \"HIGH\"\n",
        "    if score > 0.3:\n",
        "        return \"MODERATE\"\n",
        "    return \"LOW\"\n",
        "\n",
        "df_out['RiskGroup'] = df_out['Predicted_Risk_Score'].apply(risk_group)\n",
        "df_out.to_csv(out_csv, index=False)  # overwrite with risk group included\n",
        "print(\"Added RiskGroup and updated CSV:\", out_csv)\n",
        "\n",
        "# Save a JSON summary of metrics\n",
        "summary = {\n",
        "    'best_model': best_name,\n",
        "    'metrics_test': {\n",
        "        'roc_auc': results[best_name]['roc_auc'],\n",
        "        'pr_auc': results[best_name]['pr_auc'],\n",
        "        'accuracy': results[best_name]['accuracy'],\n",
        "        'precision': results[best_name]['precision'],\n",
        "        'recall': results[best_name]['recall'],\n",
        "        'f1': results[best_name]['f1']\n",
        "    },\n",
        "    'timestamp': datetime.utcnow().isoformat()\n",
        "}\n",
        "with open(os.path.join(OUTPUT_DIR, \"training_summary.json\"), \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"\\nAll done. Artifacts written to:\", OUTPUT_DIR)\n",
        "print(\"Best model:\", best_name, \"ROC-AUC:\", results[best_name]['roc_auc'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_NU7O-TgzBe",
        "outputId": "23a6b519-7802-4ada-8e13-5b51bdcc748e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added RiskGroup and updated CSV: /content/drive/MyDrive/readmission_output/readmission_predictions_with_features.csv\n",
            "\n",
            "All done. Artifacts written to: /content/drive/MyDrive/readmission_output\n",
            "Best model: HistGB ROC-AUC: 0.491992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Save your best trained model ===\n",
        "import joblib\n",
        "\n",
        "# Suppose you already have your trained best model (e.g., best_pipeline)\n",
        "# Replace best_pipeline with your actual trained pipeline/model variable\n",
        "joblib.dump(best_pipeline, \"patient_readmission_model.pkl\")\n",
        "\n",
        "print(\"Model successfully saved as 'patient_readmission_model.pkl'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIFasmVokV8c",
        "outputId": "518126a2-03c9-4a6d-d4d5-ead15a561fac"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model successfully saved as 'patient_readmission_model.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "I_wYRY-GliCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the saved model\n",
        "model = joblib.load(\"patient_readmission_model.pkl\")\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# === Helper Functions ===\n",
        "def create_age_bucket(age):\n",
        "    if pd.isna(age): return np.nan\n",
        "    if age <= 30: return '<=30'\n",
        "    if age <= 50: return '31-50'\n",
        "    if age <= 65: return '51-65'\n",
        "    if age <= 80: return '66-80'\n",
        "    return '80+'\n",
        "\n",
        "def safe_input(prompt, dtype=str):\n",
        "    \"\"\"Helper to safely take input and cast it.\"\"\"\n",
        "    val = input(prompt)\n",
        "    if dtype == float:\n",
        "        try:\n",
        "            return float(val)\n",
        "        except:\n",
        "            return np.nan\n",
        "    return val.strip()\n",
        "\n",
        "# === Collect user input ===\n",
        "print(\"\\nPlease enter patient details for readmission prediction:\\n\")\n",
        "\n",
        "Admission_ID = safe_input(\"Admission ID: \")\n",
        "Age = safe_input(\"Age (in years): \", float)\n",
        "Sex = safe_input(\"Sex (Male/Female): \")\n",
        "Weight = safe_input(\"Weight (kg): \", float)\n",
        "Admission_Date = safe_input(\"Admission Date (YYYY-MM-DD): \")\n",
        "Admission_Time = safe_input(\"Admission Time (HH:MM:SS): \")\n",
        "Doctor_ID = safe_input(\"Doctor ID: \")\n",
        "Problem_Type = safe_input(\"Problem Type (e.g., Cardiology, Neurology): \")\n",
        "Discharge_Time = safe_input(\"Discharge Time (HH:MM:SS): \")\n",
        "Blood_Pressure = safe_input(\"Blood Pressure (e.g., 120/80): \")\n",
        "Insulin = safe_input(\"Insulin (if not available, press Enter): \")\n",
        "Blood_Group = safe_input(\"Blood Group (A+, O-, etc.): \")\n",
        "Cholesterol = safe_input(\"Cholesterol (mg/dL): \", float)\n",
        "Platelets = safe_input(\"Platelets (10^9/L): \", float)\n",
        "Diabetics = safe_input(\"Diabetics (Yes/No): \")\n",
        "weather = safe_input(\"Weather condition (e.g., Clear, Rainy): \")\n",
        "air_quality_index = safe_input(\"Air Quality Index: \", float)\n",
        "social_event_count = safe_input(\"Social Event Count (last 30 days): \", float)\n",
        "Hemoglobin = safe_input(\"Hemoglobin (g/dL): \", float)\n",
        "WBC_Count = safe_input(\"WBC Count (10^9/L): \", float)\n",
        "Platelet_Count = safe_input(\"Platelet Count (10^9/L): \", float)\n",
        "Urine_Protein = safe_input(\"Urine Protein (mg/dL): \", float)\n",
        "Urine_Glucose = safe_input(\"Urine Glucose (mg/dL): \", float)\n",
        "ECG_Result = safe_input(\"ECG Result (Normal/Abnormal): \")\n",
        "Pulse_Rate = safe_input(\"Pulse Rate (bpm): \", float)\n",
        "State = safe_input(\"State: \")\n",
        "City = safe_input(\"City: \")\n",
        "Location = safe_input(\"Location: \") # Added Location input\n",
        "\n",
        "\n",
        "# === Process Inputs ===\n",
        "age_bucket = create_age_bucket(Age)\n",
        "# You'll need to implement the actual logic for comorbidity_count based on your data\n",
        "# For this example, we'll keep the placeholder or derive it if possible from collected inputs.\n",
        "# For a real application, map Diabetics and other relevant inputs to comorbidity count.\n",
        "comorbidity_count = 1.0 # placeholder\n",
        "\n",
        "\n",
        "# Extract systolic blood pressure (first value)\n",
        "try:\n",
        "    Blood_Pressure = str(Blood_Pressure).split(\"/\")[0]\n",
        "    Blood_Pressure = float(Blood_Pressure)\n",
        "except:\n",
        "    Blood_Pressure = np.nan\n",
        "\n",
        "# === Create DataFrame ===\n",
        "new_patient_data = pd.DataFrame({\n",
        "    'Admission ID': [Admission_ID],\n",
        "    'Age': [Age],\n",
        "    'Sex': [Sex],\n",
        "    'Weight': [Weight],\n",
        "    'Admission Date': [Admission_Date],\n",
        "    'Admission Time': [Admission_Time],\n",
        "    'Doctor ID': [Doctor_ID],\n",
        "    'Problem Type': [Problem_Type],\n",
        "    'Discharge Time': [Discharge_Time],\n",
        "    'Blood Pressure': [Blood_Pressure],\n",
        "    'Insulin': [Insulin],\n",
        "    'Blood Group': [Blood_Group],\n",
        "    'Cholesterol': [Cholesterol],\n",
        "    'Platelets': [Platelets],\n",
        "    'Diabetics': [Diabetics],\n",
        "    'weather': [weather],\n",
        "    'air_quality_index': [air_quality_index],\n",
        "    'social_event_count': [social_event_count],\n",
        "    'Hemoglobin (g/dL)': [Hemoglobin],\n",
        "    'WBC Count (10^9/L)': [WBC_Count],\n",
        "    'Platelet Count (10^9/L)': [Platelet_Count],\n",
        "    'Urine Protein (mg/dL)': [Urine_Protein],\n",
        "    'Urine Glucose (mg/dL)': [Urine_Glucose],\n",
        "    'ECG Result': [ECG_Result],\n",
        "    'Pulse Rate (bpm)': [Pulse_Rate],\n",
        "    'State': [State],\n",
        "    'City': [City],\n",
        "    'Location': [Location], # Added Location to DataFrame\n",
        "    'age_bucket': [age_bucket],\n",
        "    'comorbidity_count': [comorbidity_count]\n",
        "})\n",
        "\n",
        "# Convert date columns\n",
        "new_patient_data['Admission Date'] = pd.to_datetime(new_patient_data['Admission Date'], errors='coerce')\n",
        "\n",
        "# === Make Prediction ===\n",
        "prediction = model.predict(new_patient_data)[0]\n",
        "probability = model.predict_proba(new_patient_data)[:, 1][0]\n",
        "\n",
        "# === Output ===\n",
        "risk_label = \"High\" if probability > 0.7 else \"Medium\" if probability > 0.3 else \"Low\"\n",
        "\n",
        "print(\"\\n Prediction Results:\")\n",
        "print(f\"Predicted Readmission: {'Yes' if prediction == 1 else 'No'}\")\n",
        "print(f\"Readmission Probability: {probability:.2f}\")\n",
        "print(f\"Risk Level: {risk_label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noacRFGrnCmE",
        "outputId": "a7d4b144-e731-47c7-d66f-53695d5c1a98"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n",
            "\n",
            "Please enter patient details for readmission prediction:\n",
            "\n",
            "Admission ID: ADM2009\n",
            "Age (in years): 45\n",
            "Sex (Male/Female): fem ale\n",
            "Weight (kg): 56\n",
            "Admission Date (YYYY-MM-DD): 2025-09-25\n",
            "Admission Time (HH:MM:SS): 21:05:55\n",
            "Doctor ID: DOC405\n",
            "Problem Type (e.g., Cardiology, Neurology): cardiology\n",
            "Discharge Time (HH:MM:SS): 23:06:32\n",
            "Blood Pressure (e.g., 120/80): 90\n",
            "Insulin (if not available, press Enter): \n",
            "Blood Group (A+, O-, etc.): o\n",
            "Cholesterol (mg/dL): 52\n",
            "Platelets (10^9/L): 54566\n",
            "Diabetics (Yes/No): 546565\n",
            "Weather condition (e.g., Clear, Rainy): Rainy\n",
            "Air Quality Index: 52\n",
            "Social Event Count (last 30 days): 2\n",
            "Hemoglobin (g/dL): 45\n",
            "WBC Count (10^9/L): 453632165\n",
            "Platelet Count (10^9/L): 336512326\n",
            "Urine Protein (mg/dL): 52\n",
            "Urine Glucose (mg/dL): 48\n",
            "ECG Result (Normal/Abnormal): normal\n",
            "Pulse Rate (bpm): 23\n",
            "State: missouri\n",
            "City: kansascity\n",
            "Location: missouri,kansascity\n",
            "\n",
            " Prediction Results:\n",
            "Predicted Readmission: Yes\n",
            "Readmission Probability: 0.61\n",
            "Risk Level: Medium\n"
          ]
        }
      ]
    }
  ]
}